\documentclass[11pt,letterpaper]{article}

\usepackage[margin=0.8in]{geometry}

\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fixmath}
\usepackage{hyperref}

\renewcommand{\vec}[1]{\mbox{\boldmath$#1$}}
\newcommand{\tensor}[1]{\mbox{\boldmath{\ensuremath{#1}}}}

\begin{document}

\title{SMC: A Compressible Navier-Stokes Code for Combustion}
\maketitle

\section{Introduction}

{\tt SMC} {\footnote{People often ask what SMC means.  The name was
    chosen by someone who was trained as an astrophysicist.  The
    Center for Computational Sciences and Engineering (CCSE) has a low
    Mach number combustion code called LMC, which can also stand for
    Large Magellanic Cloud, a satellite galaxy of the Milky Way.  The
    LMC galaxy has a little brother called the Small Magellanic Cloud
    (SMC). The SMC code is also like a little brother of the LMC
    code.}}, developed at the Center for Computational Sciences and
Engineering (CCSE) at Lawrence Berkeley National Laboratory, is a
combustion code.  {\tt SMC} uses the {\tt BoxLib} library
(\url{https://ccse.lbl.gov/BoxLib/}), also developed at CCSE.  {\tt
  SMC} solves compressible Navier-Stokes equations for viscous
multi-component reacting flows.

\section{Basic Formulation and Algorithm}

The Navier-Stokes equations solved by {\tt SMC} are
\begin{align}
\frac{\partial \rho}{\partial t} + \nabla \cdot (\rho
    \vec{u})= { } & 0, \\
\frac{\partial \rho \vec{u}}{\partial t} + \nabla \cdot (\rho
    \vec{u}\vec{u}) + \nabla p= { } & \nabla \cdot
  \tensor{\tau}, \\
\frac{\partial \rho E}{\partial t} + \nabla \cdot [(\rho E + p)
  \vec{u}] = { } & \nabla \cdot (\lambda \nabla T) + \nabla \cdot
  (\tensor{\tau} \cdot \vec{u}) - \nabla \cdot [\sum_k\rho Y_k
  (\vec{V}_k -\vec{V}_c) h_k ], \\
\frac{\partial \rho Y_k}{\partial t} + \nabla \cdot (\rho Y_k \vec{u})
= { } & \rho \dot{\omega}_k  - \nabla \cdot [\rho Y_k
  (\vec{V}_k -\vec{V}_c) ],
\end{align}
where $\rho$ is the density, $\vec{u}$ is the velocity, $p$ is the
pressure, $E$ is the specific energy density (kinetic energy plus
internal energy), $k$ denotes species, $Y_k$ is the mass fraction of
species $k$, $\tensor{\tau}$ is the viscous stress tensor, $\lambda$
is the thermal conductivity, $T$ is the temperature, $h_k$ is
the enthalpy for species $k$, $\vec{V}_k$ is the species mass
diffusion velocity, $\vec{V}_c$ is the correction velocity for species
diffusion arisen in the mixture model, and $\dot{\omega}_k$ is the
production rate of species $k$.  The viscous stress tensor is given by
\begin{equation}
  \tau_{ij} = \eta \left(\frac{\partial u_i}{\partial x_j} +
    \frac{\partial u_j}{\partial x_i} - \frac{2}{3}
    \delta_{ij} \nabla \cdot \vec{u} \right) +
  \xi \delta_{ij} \nabla \cdot \vec{u},
\end{equation}
where $\eta$ is the shear viscosity and $\xi$ is the bulk viscosity.
A mixture model is employed in {\tt SMC}.  The species diffusion
velocity in the mixture model is given by
\begin{equation}
  \vec{V}_k = -D_{k} \nabla X_k - D_{k} (X_k-Y_k)
  \nabla (\ln{p}),
\end{equation}
where $D_k$ is the diffusion coefficient of species $k$ and $X_k$ is
the molar fraction of species $k$.  The correction velocity is given by
\begin{equation}
  \vec{V}_c = \sum_\ell Y_\ell \vec{V}_\ell.
\end{equation}

The {\tt SMC} algorithm is based on finite-difference methods.  For
first derivatives with respect to spatial coordinates, the standard
8th-order stencil is employed.  Using a new narrow 8th-order stencil,
second derivatives are given by
\begin{equation}
\frac{\partial}{\partial x} \left(a(x) \frac{\partial u}{\partial x}
\right)\bigg{|}_{i} \approx \frac{H_{i+1/2} - H_{i-1/2}}{\Delta x^2}.
\end{equation}
Here
\begin{equation}
  H_{i+1/2} = \sum_{m=-3}^{4} \sum_{n=-3}^{4} a_{i+m} M_{mn} u_{i+n},
\end{equation}
where $M$ is a $8 \times 8$ matrix.  {\tt SMC} also has a {\tt S3D}
style algorithm for second derivatives, in which the first derivative
operator is applied twice to obtain a second derivative.

The system is evolved with either a 3rd-order Runge-Kutta method or a
spectral deferred correction (SDC) method.  {\tt SMC} uses
Navier-Stokes characteristic boundary conditions (NSCBC).  The cgs
units are used in {\tt SMC}.

\section{Getting Started}

{\tt SMC} requires the {\tt BoxLib} library to manage the grids,
parallelization, and compilation system.  It uses the {\tt
  BOXLIB\_HOME} environment variable to find the path to {\tt BoxLib}.
{\tt BoxLib} can be downloaded via {\tt git} as: \vspace{5pt}

\verb|git clone https://ccse.lbl.gov/pub/Downloads/BoxLib.git|\\

We will use the {\tt SMC/bin/ToyFlame} problem as an example of
running {\tt SMC}.  The {\tt ToyFlame} directory contains a file
called {\tt GNUmakefile} that specify how to build the executable.
Note GNU {\tt make} is required for building.  The {\tt GNUmakefile}
has the following options:
\begin{itemize}
\item {\tt SMC\_SRC := src\_tb\_min}\\
  There are several versions of SMC.  This option determines which
  version we use.  The possible values are {\tt src}, {\tt
    src\_tb\_min} and {\tt src\_min}.  The {\tt src\_min} version is
  now obsolete.  The {\tt src\_tb\_min} version only supports triple
  periodic boundary conditions, and it only uses RK3 for time
  stepping.  This is used as a proxy app for code analysis. The {\tt
    src} version is the most sophisticated one.  It uses NSCBC for
  non-periodic boundaries.  It can use either SDC or RK3 for time
  stepping.  The {\tt src} version requires an additional
  softwarepackage, {\tt SDCLib}, available from Matthew Emmett ({\tt
    mwemmett@lbl.gov}) of CCSE upon request.
\item {\tt K\_USE\_STACK := t}\\
  This option is for the {\tt src\_tb\_min} version only. There are
  several versions of {\tt kernels.f90}.  If this is set, either {\tt
    kernels\_stack\_exp.f90} or {\tt kernels\_stack.f90} will be used
  depending on the {\tt EXPAND} option (see next item).  Otherwise,
  either {\tt kernels\_exp.f90} or {\tt kernels.f90} will be used.
  The {\tt stack} versions are slightly faster.
\item {\tt EXPAND := t}\\
  There are several versions of {\tt kernels.f90}.  Some were written
  by hand, whereas {\tt kernels\_exp.f90} and {\tt
    kernels\_stack\_exp.f90} were generated by {\tt
    SMC/tools/expand.py}.  The {\tt exp} versions are much faster.
  This option determines which version we use.
\item {\tt NDEBUG :=}\\
  This option determines whether we compile with support for debugging
  (usually also enabling some runtime checks) and turn off
  optimization. Setting NDEBUG := t turns off debugging and turns on
  optimization. 
\item {\tt PROF :=}\\
  This option determines whether we compile with timers. Leaving this
  option empty disables timers.
\item {\tt MPI := t}\\
  This determines whether we are doing a parallel build, using the
  Message Passing Interface (MPI) library. Leaving this
  option empty will disable MPI.
\item {\tt OMP :=}\\
  This determines whether we are using OpenMP to do parallelism within
  a shared memory node. OpenMP is used together with MPI, with MPI
  distributing the grids across the processors and within a
  shared-memory node, OpenMP allows many cores to operate on the same
  grid.  Leaving this option empty disables OpenMP.
\item {\tt COMP := gfortran}\\
  Set this to your Fortran compiler of choice (e.g., gfortran, PGI,
  Intel, or Cray).
\item {\tt MKVERBOSE := t}\\
  This determines the verbosity of the building process.
\end{itemize}

The {\tt BoxLib} building system used by {\tt SMC} is located at {\tt
  \$\{BOXLIB\_HOME\}/Tools/F\_mk}.  The compiler flags are specified
in {\tt \$\{BOXLIB\_HOME\}/Tools/F\_mk/comps/}.  If building with MPI,
{\tt SMC} needs to know the location of MPI library.  You can set {\tt
  BOXLIB\_USE\_MPI\_WRAPPERS=1} in the environment to make {\tt
  BoxLib} use {\tt mpif90}.  Otherwise, you need to provide the MPI
locations in the {\tt F\_mk/GMakeMPI.mak} file.  Note that some shells
(e.g., {\tt tcsh}) use the {\tt \$HOST} environment variable to store
host name, whereas other shells (e.g., {\tt bash}) use {\tt
  \$HOSTNAME}.

To build an executable, type {\tt make} in the {\tt ToyFlame/}
directory.  The executable will have a name like {\tt
  main.Linux.gfortran.debug.mpi.exe} depending on the compiler,
operating system, and some other options specified in the {\tt
  GNUmakefile}.  To run {\tt SMC} on a single processor, type
\vspace{5pt}

\verb|./main.Linux.gfortran.debug.mpi.exe inputs_SMC|\vspace{5pt}\\
in the {\tt ToyFlame/} directory, where the {\tt inputs\_SMC} file
specifies runtime parameters.  If OpenMP is used, you may need to
increase the thread stack size (see \S~\ref{sec:OMP_STACKSIZE}).

\section{Runtime Parameters}

{\tt SMC} obtains runtime parameters from an inputs file.  We can also
override the value of any parameter by specifying it on the
command line as:\vspace{5pt}

\verb|./main.Linux.gfortran.debug.mpi.exe inputs_SMC --parameter value|\\

Here {\tt parameter} is the name of the runtime parameter (e.g., {\tt
  stop\_time}) and {\tt value} is its value (e.g., {\tt 1.0d-4}).
Some selected runtime parameters and their default values are listed
below.  Note that some of the default values (e.g., {\tt n\_cellx =
  -1}) are intentionally set to improper values so that the user must
set them appropriately.

\subsection{Grid}

\begin{itemize}
\item {\tt n\_cellx = -1}\\
  Number of cells in the $x$-direction.  It must be $\ge 4$.
\item {\tt n\_celly = -1}\\
  Number of cells in the $y$-direction.  It must be $\ge 4$.
\item {\tt n\_cellz = -1}\\
  Number of cells in the $z$-direction.  It must be $\ge 4$.
\item {\tt max\_grid\_size = 64}\\
  This determines the largest grid size of a box.  If {\tt
    max\_grid\_size} is too big, there might be fewer boxes than MPI
  tasks, and then some processors will be idle.  However, if it is too
  small, each MPI task may have too many small boxes, and the
  performance will be affected.  Thus, {\tt max\_grid\_size} must be
  chosen according to the number of cells and the number of MPI tasks.
  Ideally, you would like to have one box for each MPI task to
  minimize the communication cost.
\item {\tt prob\_lo\_x = 0.d0}\\
  Physical coordinates of lo-$x$ corner of problem domain
\item {\tt prob\_lo\_y = 0.d0}\\
  Physical coordinates of lo-$y$ corner of problem domain
\item {\tt prob\_lo\_z = 0.d0}\\
  Physical coordinates of lo-$z$ corner of problem domain
\item {\tt prob\_hi\_x = 1.d0}\\
  Physical coordinates of hi-$x$ corner of problem domain
\item {\tt prob\_hi\_y = 1.d0}\\
  Physical coordinates of hi-$y$ corner of problem domain
\item {\tt prob\_hi\_z = 1.d0}\\
  Physical coordinates of hi-$z$ corner of problem domain
\end{itemize}

\subsection{Timestepping}

\begin{itemize}
\item {\tt stop\_time = -1.d0}\\
  Simulation stop time.
\item {\tt max\_step = 1}\\
  Maximum number of timesteps in the simulation.
\item {\tt cflfac = 0.5d0}\\
  CFL factor to use in the computation of the advection timestep
  constraint.
\item {\tt cfl\_int = 10}\\
  Number of steps between computing the Courant number.  Since the
  system changes very slowly over steps for an explicit combustion
  code, it is unnecessary to compute the Courant number very often.
\item {\tt init\_shrink = 1.d0}\\
  The multiplicative factor ($\le 1$) to reduce the initial timestep
  as computed by the CFL constraint.
\item {\tt small\_dt = 1.d-30}\\
  The minimum allowed timestep.  We abort if dt drops below this
  value.
\item {\tt max\_dt = 1.d33}\\
  This is the maximum dt that is allowed
\item {\tt fixed\_dt = -1.0d0}\\
  Fix the time step if it is greater than zero.  If it is less than or
  equal to zero, use the standard time step determined by the CFL
  condition.
\end{itemize}

\subsection{Input/Output}

\begin{itemize}
\item {\tt restart = -1}\\
  This determines whether this is a restart run.  The value of -1
  means the simulation starts from scratch.  A non-negative value
  specifies which file to restart from.
\item {\tt plot\_int = 0}\\
  Number of timesteps between writing a plot file, if the value is
  positive.
\item {\tt plot\_deltat = -1.d0} \\
  If this is greater than zero, dump a plotfile after the solution has
  advanced past {\tt plot\_deltat} in time.
\item {\tt chk\_int = 0}\\
  Number of timesteps between writing a checkpoint file, if the value is
  positive.
\item {\tt job\_name = ""}\\
  Job name printed in output.
\item {\tt plot\_Y = .true.}\\
  Plot mass fraction in plotfile.
\item {\tt plot\_X = .false.}\\
  Plot molar fraction in plotfile.
\item {\tt plot\_eint = .false.}\\
  Plot mean specific internal energy in plotfile.
\item {\tt plot\_h = .true.}\\
  Plot mean specific enthlpy in plotfile.
\item {\tt plot\_hspec = .false.}\\
  Plot enthlpy of each species in plotfile.
\item {\tt plot\_divu = .true.}\\
  Plot divergence of velocity in plotfile.
\item {\tt plot\_omegadot = .true.}\\
  Plot mass production rate for each species in plotfile.
\item {\tt plot\_base\_name = "plt"}\\
  Prefix to use in plotfile names
\item {\tt check\_base\_name = "chk"}\\
  Prefix to use in checkpoint file names
\item {\tt single\_prec\_plotfiles = .false.}\\
  Store the state data in single precision if true.
\end{itemize}

\subsection{Communication}

\begin{itemize}
\item {\tt overlap\_comm\_comp = .false.}\\
  This determines whether it will try to overlap communication and
  computation.  For the overlap to work, the MPI non-blocking send and
  receive need to be asynchronous.  However, many MPI implementations
  do not actually perform any communication until {\tt MPI\_Wait} is
  called.  In that case, communication hiding would not work.
\item {\tt overlap\_comm\_gettrans = .false.} \\
  This determines whether the computation of transport coefficients is
  included in the computation that might be overlapped with
  communication.
\item {\tt overlap\_testing = .false.} \\
  When this is set to true, a test is performed at the beginning of
  the run to determine whether it is worth overlapping communication
  and computation.  If the test shows no benefit due to the overlap,
  the {\tt overlap\_comm\_comp} parameter will be set to false and the
  overlap of communication and computation will be turned off.
\end{itemize}

\subsection{Box Layout}
\begin{itemize}
\item {\tt t\_trylayout = -1.d0} \\
  When this is greater than zero, a number of random permutations of
  the layout of the boxes will be performed and communication tests
  will be carried out to find the best layout for communication.  This
  parameter also determines the maximal time in second used in the
  testing.
\item {\tt overlap\_in\_trial = .false.}\\
  Is communication and computation overlapping used when trying
  layouts?  The default is set to false so that more layouts can be
  tried within a certain amount of time.
\end{itemize}

\subsection{Thread Parallelization and Blocking}

In many subroutines in SMC, a box is divided into virtual boxes, and
each thread works one box.  (It is just a special case with one thread
when OpenMP is not used.)  Each ``threadbox'' is further divided into
blocks.  The blocking makes the code more cache friendly. 

\begin{itemize}
\item {\tt tb\_split\_dim = 2}\\
  This determines how many dimensions of a box is divided into
  threadboxes.  For example, a box will be divided into pencils, when
  {\tt tb\_split\_dim = 2}.
\item {\tt tb\_collapse\_boxes = .false.}\\
  If this is true, boxes on each processor are ``collapsed'' before
  being split into threadboxes.
\item {\tt tb\_idim\_more = 2}\\
  This parameter determines which dimension is split more into
  threadboxes.  Suppose that there are 8 threads and the boxes will be
  split in two dimensions.  Note that $8 = 4 \times 2$.  Setting {\tt
    tb\_idim\_more = 2} will split the second dimension (i.e.,
  y-direction) of the boxes into 4 parts.
\item {\tt tb\_idim\_less = 1}\\
  This parameter determines which dimension is split less into
  threadboxes.  This is typically 1 (i.e., x-direction), because
  multi-dimensional arrays in {\tt SMC} are contiguous in memory in
  x-direction.
\item {\tt tb\_blocksize\_x = -1}\\
  This is the targeted size of blocks in x-direction.  A negative
  value means no dividing in this direction.
\item {\tt tb\_blocksize\_y = 16}\\
  This is the targeted size of blocks in y-direction.  A negative
  value means no dividing in this direction.
\item {\tt tb\_blocksize\_z = 16}\\
  This is the targeted size of blocks in z-direction.  A negative
  value means no dividing in this direction.
\end{itemize}

\subsection{Algorithm}

\begin{itemize}
\item {\tt advance\_method = 1}\\
  The choices are 1 (3rd-order Runge-Kutta), 2 (SDC), and 3
  (Multi-Rate SDC).
% \item {\tt stencil\_type = "narrow"}\\
%   This determines the stencil type for second derivatives. For the
%   {\tt src\_min} version, the options are {\tt "narrow"} or {\tt
%     "S3D"}. However, it must be {\tt narrow} for other versions.
\end{itemize}

\subsection{SDC}

\begin{itemize}
\item {\tt sdc\_nnodes = 3}\\
  Number of SDC nodes.  Valid choices are any positive integer greater
  than 2.
\item {\tt sdc\_iters = 5}\\
  Number of SDC iterations per time step.  Valid choices are any
  positive integer.
\item {\tt sdc\_tol\_residual = -1.0d100}\\
  If the successive ratios of SDC residuals, relative to unity, drop
  below this tolerance then SDC iterations are cut short.  If this
  tolerance is negative, then SDC iterations are never cut short.
  More formally, if
  \begin{equation*}
    \left| \frac{||R_{k-1}||_2}{||R_k||_2} - 1 \right| < \text{tolerance}
  \end{equation*}
  where $R_k$ is the SDC residual of the $k^{\rm th}$ iteration, then
  the iterations stop.
\end{itemize}

If SDC time-stepping is used, the number of SDC nodes used and the
number of SDC iterations (sweeps) taken, per time-step, can be
adjusted.  The type of SDC nodes that {\tt SMC} uses is (currently)
Gauss-Lobatto.  Therefore, the formal order of accuracy of the
resulting time-stepping scheme is the minimum of $2{\tt sdc\_nnodes} -
2$ and ${\tt sdc\_iters}$.  For example, with 5 Gauss-Lobatto
nodes the formal order of accuracy is 8 as long as 8 iterations are
taken.  If only 6 iterations are taken, then the formal order of
accuracy is reduced to 6.

Note that a time-stepping scheme of arbitrary order can be built by
simply adjusting {\tt sdc\_nnodes} and {\tt sdc\_iters}.  Keep in
mind, however, that these parameters also effect the stability of the
resulting scheme.

\subsection{Physics and Chemistry}
\begin{itemize}
\item {\tt reset\_inactive\_species = .false.}\\
  The parameter determines how to handle the diffusion correction
  velocity in the mixture model.  If it is {\tt .false.}, correction
  will be applied to all species; otherwise, the ``inactive species''
  is reset to satisfy the mass conservation.
\item {\tt inactive\_species\_name = N2"}\\
  This is the name of the ``inactive species''.
\item {\tt use\_bulk\_viscosity = .true.}\\
  This determines is bulk viscosity is used.
\end{itemize}


\subsection{Boundary Conditions}

\begin{itemize}
\item {\tt xlo\_boundary\_type = "periodic"}\\
  $-x$ boundary condition.  Other valid choices are {\tt "inlet"} and
  {\tt "outlet"}.
\item {\tt xhi\_boundary\_type = "periodic"}\\
  $+x$ boundary condition.  The other valid choice is {\tt "outlet"}.
\item {\tt ylo\_boundary\_type = "periodic"}\\
  $-y$ boundary condition.  The other valid choice is {\tt "outlet"}.
\item {\tt yhi\_boundary\_type = "periodic"}\\
  $+y$ boundary condition.  The other valid choice is {\tt "outlet"}.
\item {\tt zlo\_boundary\_type = "periodic"}\\
  $-z$ boundary condition.  The other valid choice is {\tt "outlet"}.
\item {\tt zhi\_boundary\_type = "periodic"}\\
  $+z$ boundary condition.  The other valid choice is {\tt "outlet"}.
\item {\tt outlet\_sigma = 0.25d0}\\
  The $\sigma$ parameter in NSCBC for outflows.
\item {\tt outlet\_eta = 50.d0}\\
  The $\eta$ parameter in NSCBC for outflows.
\item {\tt outlet\_Pinfty = 1.01325d6}\\
  $P_\infty$ in NSCBC.
\item {\tt inlet\_eta = 30.d0}\\
  The $\eta$ parameter in NSCBC for inflows.
\end{itemize}

\subsection{Miscellaneous}

\begin{itemize}
\item {\tt verbose = 0}\\
  This determines verbosity.
\end{itemize}

\section{Tips}

\subsection{Tag File}

You can generate a tag file for {\tt vi} by typing ``{\tt make
  tags}'', and for {\tt emacs} by typing ``{\tt make TAGS}''.  Tags
can help you navigate the source code more quickly.

\subsection{Runtime Interaction}

You can interact with the program during the runtime by typing certain
messages.  You can ask the program to dump a checkpoint by creating a
file called {\tt dump\_checkpoint} (e.g., typing ``{\tt touch
  dump\_checkpoint}'') in the output directory, or a plotfile by
typing ``{\tt touch dump\_plotfile}''.  You can force the program to
abort by typing ``{\tt touch abort\_smc}''.  This has the effect of
also dumping a final checkpoint file.

\subsection{OpenMP Stack Size}
\label{sec:OMP_STACKSIZE}

The default stack size of OpenMP threads may not be enough for large
runs.  If that happens, the code will crash due to stack overflow, and
you could increase the stack size by setting the {\tt OMP\_STACKSIZE}
environment variable to a large value (e.g., {\tt 100M}).  You may
also need to increase the stack size of your shell to unlimited.

% \section{Machine Specific Notes}

% \subsection{Hopper}

% On Hopper, Cray compilers generate the fastest executable for {\tt
%   SMC}.  The default setting for Cray Compilers in {\tt BoxLib} is
% very conservative even when {\tt NDEBUG} is true.  You can change it
% from {\tt -O 1} to {\tt -O3 -hfp3} in the {\tt Linux\_cray.mak} file
% under the {\tt BoxLib/Tools/F\_mk/comps/} directory.

% You can run {\tt SMC} on Hopper using either pure MPI or hybrid
% MPI/OpenMP.  The difference in performance does not seems to be big.
% Weak scaling studies show that the scaling behavior of SMC is
% excellent on Hopper.  It is worth noting again that you would like to
% have one box for each MPI task to minimize the communication cost.
% You might also want to set {\tt t\_trylayout = 600.d0} to find a good
% layout of boxes.

\end{document}
